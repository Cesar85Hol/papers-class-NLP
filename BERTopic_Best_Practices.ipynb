{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo-Oig4Yib5K"
      },
      "source": [
        "# **Tutorial** - BERTopic Best Practices\n",
        "\n",
        "Through the nature of BERTopic, its modularity, many variations of the topic modeling technique is possible. However, during the development and through the usage of the package, a set of best practices have been developed that generally lead to great results.\n",
        "\n",
        "The following are a number of steps, parameters, and settings that you can use that will generally improve the quality of the resulting topics. In other words, after going through the quick start and getting a feeling for the API these steps should get you to the next level of performance.\n",
        "\n",
        "**NOTE:**\n",
        "    Although these are called *best practices*, it does not necessarily mean that they work across all use cases perfectly. The underlying modular nature of BERTopic is meant to take different use cases into account. After going through these practices it is advised to fine-tune wherever necessary.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaartenGr/BERTopic/master/images/logo.png\" width=\"20%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPP4HfpJixEK"
      },
      "source": [
        "# Enabling the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "[Reference](https://colab.research.google.com/notebooks/gpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5RBp4gFuSaR"
      },
      "source": [
        "# **Installing BERTopic**\n",
        "\n",
        "We start by installing BERTopic from PyPi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mNWv-K-xiZsA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install bertopic\n",
        "!pip install datasets\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qdqzq2muUm5"
      },
      "source": [
        "## Restart the Notebook\n",
        "After installing BERTopic, some packages that were already loaded were updated and in order to correctly use them, we should now restart the notebook.\n",
        "\n",
        "From the Menu:\n",
        "\n",
        "Runtime â†’ Restart Runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vCrsZ1XuZTF"
      },
      "source": [
        "# Data\n",
        "For this example, we will use a dataset containing abstracts and metadata from [ArXiv articles](https://huggingface.co/datasets/arxiv_dataset).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE67L83cuLzs",
        "outputId": "c0f897d0-2bfc-4739-d46d-ae513abc1e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  In a sensor network, in practice, the communication among sensors is subject\n",
            "to:(1) errors or failures at random times; (3) costs; and(2) constraints since\n",
            "sensors and networks operate under scarce resources, such as power, data rate,\n",
            "or communication. The signal-to-noise ratio (SNR) is usually a main factor in\n",
            "determining the probability of error (or of communication failure) in a link.\n",
            "These probabilities are then a proxy for the SNR under which the links operate.\n",
            "The paper studies the problem of designing the topology, i.e., assigning the\n",
            "probabilities of reliable communication among sensors (or of link failures) to\n",
            "maximize the rate of convergence of average consensus, when the link\n",
            "communication costs are taken into account, and there is an overall\n",
            "communication budget constraint. To consider this problem, we address a number\n",
            "of preliminary issues: (1) model the network as a random topology; (2)\n",
            "establish necessary and sufficient conditions for mean square sense (mss) and\n",
            "almost sure (a.s.) convergence of average consensus when network links fail;\n",
            "and, in particular, (3) show that a necessary and sufficient condition for both\n",
            "mss and a.s. convergence is for the algebraic connectivity of the mean graph\n",
            "describing the network topology to be strictly positive. With these results, we\n",
            "formulate topology design, subject to random link failures and to a\n",
            "communication cost constraint, as a constrained convex optimization problem to\n",
            "which we apply semidefinite programming techniques. We show by an extensive\n",
            "numerical study that the optimal design improves significantly the convergence\n",
            "speed of the consensus algorithm and can achieve the asymptotic performance of\n",
            "a non-random network at a fraction of the communication cost.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\")[\"train\"]\n",
        "\n",
        "# Extract abstracts to train on and corresponding titles\n",
        "abstracts = dataset[\"abstract\"]\n",
        "titles = dataset[\"title\"]\n",
        "print(abstracts[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "R_BoAGzxwNHd",
        "outputId": "08ee546a-ef55-48fb-b1f0-49addbc89dd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'  The problem of statistical learning is to construct a predictor of a random\\nvariable $Y$ as a function of a related random variable $X$ on the basis of an\\ni.i.d. training sample from the joint distribution of $(X,Y)$. Allowable\\npredictors are drawn from some specified class, and the goal is to approach\\nasymptotically the performance (expected loss) of the best predictor in the\\nclass. We consider the setting in which one has perfect observation of the\\n$X$-part of the sample, while the $Y$-part has to be communicated at some\\nfinite bit rate. The encoding of the $Y$-values is allowed to depend on the\\n$X$-values. Under suitable regularity conditions on the admissible predictors,\\nthe underlying family of probability distributions and the loss function, we\\ngive an information-theoretic characterization of achievable predictor\\nperformance in terms of conditional distortion-rate functions. The ideas are\\nillustrated on the example of nonparametric regression in Gaussian noise.\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abstracts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Yfzp0lw6bL"
      },
      "source": [
        "**ðŸ”¥ Tip - Sentence Splitter ðŸ”¥**\n",
        "***\n",
        " Whenever you have large documents, you typically want to split them up into either paragraphs or sentences. A nice way to do so is by using NLTK's sentence splitter which is nothing more than:\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sentences = [sent_tokenize(abstract) for abstract in abstracts]\n",
        "sentences = [sentence for doc in sentences for sentence in doc]\n",
        "```\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBMyyzE7Qce4",
        "outputId": "a4f53db8-a855-4592-8e38-30e294be60f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/cesaraugustoseminarioyrigoyen/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentences = [sent_tokenize(abstract) for abstract in abstracts]\n",
        "sentences = [sentence for doc in sentences for sentence in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SZ--1tG1lUH"
      },
      "source": [
        "# **Best Practices**\n",
        "\n",
        "With feedback from the community throughout the development of BERTopic and the core maintainer's personal experience, there are a number of best practices developed that generally lead to an improved topic model.\n",
        "\n",
        "The goal of these best practices to quickly guide the user to what is commonly used to speed-up training, improve performance, explore alternatives, etc. Instead of having to search through many issues and discussions, an overview of best practices are discussed here.\n",
        "\n",
        "To start off, it is important to have a general idea of the pipeline of BERTopic as it relates to many of these best practices.\n",
        "\n",
        "BERTopic can be viewed as a sequence of steps to create its topic representations. There are five steps to this process:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h53uJPBbzG-t"
      },
      "source": [
        "![https://maartengr.github.io/BERTopic/algorithm/default.svg](https://maartengr.github.io/BERTopic/algorithm/default.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw0p_OpOHpoc"
      },
      "source": [
        "The pipeline above implies significant modularity of BERTopic. Each step in this process was carefully selected such that they are all somewhat independent from one another.\n",
        "\n",
        "As a result, we can adopt the pipeline to the current state-of-the-art with respect to each individual step:\n",
        "\n",
        " ![https://maartengr.github.io/BERTopic/algorithm/modularity.svg](https://maartengr.github.io/BERTopic/algorithm/modularity.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVfnYtUaxyLT"
      },
      "source": [
        "## **Pre-calculate Embeddings**\n",
        "After having created our data, namely `abstracts`, we can dive into the very first best practice, **pre-calculating embeddings**.\n",
        "\n",
        "BERTopic works by converting documents into numerical values, called embeddings. This process can be very costly, especially if we want to iterate over parameters. Instead, we can calculate those embeddings once and feed them to BERTopic to skip calculating embeddings each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTwXxSnPupbZ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Pre-calculate embeddings\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPhSGuPPJ5st"
      },
      "outputs": [],
      "source": [
        "embeddings = embedding_model.encode(abstracts, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28_EVoOfyZLb"
      },
      "source": [
        "## **Preventing Stochastic Behavior**\n",
        "In BERTopic, we generally use a dimensionality reduction algorithm to reduce the size of the embeddings. This is done to prevent the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) to a certain degree.\n",
        "\n",
        "As a default, this is done with [UMAP](https://github.com/lmcinnes/umap) which is an incredible algorithm for reducing dimensional space. However, by default, it shows stochastic behavior which creates different results each time you run it. To prevent that, we will need to set a `random_state` of the model before passing it to BERTopic.\n",
        "\n",
        "As a result, we can now fully reproduce the results each time we run the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbI9VptGxop4"
      },
      "outputs": [],
      "source": [
        "from umap import UMAP\n",
        "\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH6vZPGU2zpg"
      },
      "source": [
        "## **Controlling Number of Topics**\n",
        "There is a parameter to control the number of topics, namely `nr_topics`. This parameter, however, merges topics **after** they have been created. It is a parameter that supports creating a fixed number of topics.\n",
        "\n",
        "However, it is advised to control the number of topics through the cluster model which is by default HDBSCAN. HDBSCAN has a parameter, namely `min_topic_size` that indirectly controls the number of topics that will be created.\n",
        "\n",
        "A higher `min_topic_size` will generate fewer topics and a lower `min_topic_size` will generate more topics.\n",
        "\n",
        "Here, we will go with `min_topic_size=40` to get around XXX topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swmkcx5S3e9m"
      },
      "outputs": [],
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66zgeCyf0jy3"
      },
      "source": [
        "## **Improving Default Representation**\n",
        "The default representation of topics is calculated through [c-TF-IDF](https://maartengr.github.io/BERTopic/algorithm/algorithm.html#5-topic-representation). However, c-TF-IDF is powered by the [CountVectorizer](https://maartengr.github.io/BERTopic/getting_started/vectorizers/vectorizers.html) which converts text into tokens. Using the CountVectorizer, we can do a number of things:\n",
        "\n",
        "* Remove stopwords\n",
        "* Ignore infrequent words\n",
        "* Increase\n",
        "\n",
        "In other words, we can preprocess the topic representations **after** documents are assigned to topics. This will not influence the clustering process in any way.\n",
        "\n",
        "Here, we will ignore English stopwords and infrequent words. Moreover, by increasing the n-gram range we will consider topic representations that are made up of one or two words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyIFi06Vzeg3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIu9afMo1YYg"
      },
      "source": [
        "## **Additional Representations**\n",
        "Previously, we have tuned the default representation but there are quite a number of [other topic representations](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html) in BERTopic that we can choose from. From [KeyBERTInspired](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired) and [PartOfSpeech](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#partofspeech), to [OpenAI's ChatGPT](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#chatgpt) and [open-source](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html#langchain) alternatives, many representations are possible.\n",
        "\n",
        "In BERTopic, you can model many different topic representations simultanously to test them out and get different perspectives of topic descriptions. This is called [multi-aspect](https://maartengr.github.io/BERTopic/getting_started/multiaspect/multiaspect.html) topic modeling.\n",
        "\n",
        "Here, we will demonstrate a number of interesting and useful representations in BERTopic:\n",
        "\n",
        "* KeyBERTInspired\n",
        "  * A method that derives inspiration from how KeyBERT works\n",
        "* PartOfSpeech\n",
        "  * Using SpaCy's POS tagging to extract words\n",
        "* MaximalMarginalRelevance\n",
        "  * Diversify the topic words\n",
        "* OpenAI\n",
        "  * Use ChatGPT to label our topics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaxb00nfzejc"
      },
      "outputs": [],
      "source": [
        "!pip install typing-extensions --upgrade\n",
        "\n",
        "import openai\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
        "\n",
        "# KeyBERT\n",
        "keybert_model = KeyBERTInspired()\n",
        "\n",
        "# Part-of-Speech\n",
        "pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
        "\n",
        "# MMR\n",
        "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
        "\n",
        "# GPT-3.5\n",
        "prompt = \"\"\"\n",
        "I have a topic that contains the following documents:\n",
        "[DOCUMENTS]\n",
        "The topic is described by the following keywords: [KEYWORDS]\n",
        "\n",
        "Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
        "topic: <topic label>\n",
        "\"\"\"\n",
        "client = openai.OpenAI(api_key=\"sk-...\")\n",
        "openai_model = OpenAI(client, model=\"gpt-3.5-turbo\", exponential_backoff=True, chat=True, prompt=prompt)\n",
        "\n",
        "# All representation models\n",
        "representation_model = {\n",
        "    \"KeyBERT\": keybert_model,\n",
        "    # \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n",
        "    \"MMR\": mmr_model,\n",
        "    \"POS\": pos_model\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yjoDpUxDItK"
      },
      "source": [
        "## **Training**\n",
        "Now that we have a set of best practices, we can use them in our training loop. Here, several different representations, keywords and labels for our topics will be created. If you want to iterate over the topic model it is advised to use the pre-calculated embeddings as that significantly speeds up training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3K4ehYozTBZ"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic(\n",
        "\n",
        "  # Pipeline models\n",
        "  embedding_model=embedding_model,\n",
        "  umap_model=umap_model,\n",
        "  hdbscan_model=hdbscan_model,\n",
        "  vectorizer_model=vectorizer_model,\n",
        "  representation_model=representation_model,\n",
        "\n",
        "  # Hyperparameters\n",
        "  top_n_words=10,\n",
        "  verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(abstracts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3WRXoRP2ej8"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUYo2tokvhCL"
      },
      "source": [
        "To get all representations for a single topic, we simply run the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmfKtFIcvkrx"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic(1, full=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZeQ32r8CeNi"
      },
      "source": [
        "**NOTE**: The labels generated by OpenAI's **ChatGPT** are especially interesting to use throughout your model. Below, we will go into more detail how to set that as a custom label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPk6VfFh2hOR"
      },
      "source": [
        "**ðŸ”¥ Tip - Parameters ðŸ”¥**\n",
        "***\n",
        "If you would like to return the topic-document probability matrix, then it is advised to use `calculate_probabilities=True`. Do note that this can significantly slow down training. To speed it up, use [cuML's HDBSCAN](https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html#cuml-hdbscan) instead. You could also approximate the topic-document probability matrix with `.approximate_distribution` which will be discussed later.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2bto3BLB3ks"
      },
      "source": [
        "## **(Custom) Labels**\n",
        "The default label of each topic are the top 3 words in each topic combined with an underscore between them.\n",
        "\n",
        "This, of course, might not be the best label that you can think of for a certain topic. Instead, we can use `.set_topic_labels` to manually label all or certain topics.\n",
        "\n",
        "We can also use `.set_topic_labels` to use one of the other topic representations that we had before, like `KeyBERTInspired` or even `OpenAI`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meEM585mCk7a"
      },
      "outputs": [],
      "source": [
        "# Label the topics yourself\n",
        "topic_model.set_topic_labels({1: \"Space Travel\", 7: \"Religion\"})\n",
        "\n",
        "# or use one of the other topic representations, like KeyBERTInspired\n",
        "keybert_topic_labels = {topic: \" | \".join(list(zip(*values))[0][:3]) for topic, values in topic_model.topic_aspects_[\"KeyBERT\"].items()}\n",
        "topic_model.set_topic_labels(keybert_topic_labels)\n",
        "\n",
        "# or ChatGPT's labels\n",
        "chatgpt_topic_labels = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()}\n",
        "chatgpt_topic_labels[-1] = \"Outlier Topic\"\n",
        "topic_model.set_topic_labels(chatgpt_topic_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPHq4EHayanF"
      },
      "source": [
        "Now that we have set the updated topic labels, we can access them with the many functions used throughout BERTopic. Most notably, you can show the updated labels in visualizations with the `custom_labels=True` parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8KZTeb6vRgo"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyBVZIt7y6Q5"
      },
      "source": [
        "Notice that the overview in `.get_topic_info` now also includes the column `CustomName`. That is the custom label that we just created for each topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4GFP1c8Ex5T"
      },
      "source": [
        "## **Topic-Document Distribution**\n",
        "If using `calculate_probabilities=True` is not possible, than you can [approximate the topic-document distributions](https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html) using `.approximate_distribution`. It is a fast and flexible method for creating different topic-document distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lu_XyUEEyJH"
      },
      "outputs": [],
      "source": [
        "# `topic_distr` contains the distribution of topics in each document\n",
        "topic_distr, _ = topic_model.approximate_distribution(abstracts, window=8, stride=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT0lFY0F0FHc"
      },
      "source": [
        "Next, lets take a look at a specific abstract and see how the topic distribution was extracted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIHHfgiEz1hZ"
      },
      "outputs": [],
      "source": [
        "abstract_id = 10\n",
        "print(abstracts[abstract_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgQ5b_B0FYw6"
      },
      "outputs": [],
      "source": [
        "# Visualize the topic-document distribution for a single document\n",
        "topic_model.visualize_distribution(topic_distr[abstract_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRP2BEbSFMPE"
      },
      "outputs": [],
      "source": [
        "# Visualize the topic-document distribution for a single document\n",
        "topic_model.visualize_distribution(topic_distr[abstract_id], custom_labels=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iClmuzqP0PXc"
      },
      "source": [
        "It seems to have extracted a number of topics that are relevant and shows the distributions of these topics across the abstract. We can go one step further and visualize them on a token-level:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDM4cxYSFhPG"
      },
      "outputs": [],
      "source": [
        "# Calculate the topic distributions on a token-level\n",
        "topic_distr, topic_token_distr = topic_model.approximate_distribution(abstracts[abstract_id], calculate_tokens=True)\n",
        "\n",
        "# Visualize the token-level distributions\n",
        "df = topic_model.visualize_approximate_distribution(abstracts[abstract_id], topic_token_distr[0])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPanQiUfE2_z"
      },
      "source": [
        "**ðŸ”¥ Tip - `use_embedding_model` ðŸ”¥**\n",
        "***\n",
        "As a default, we compare the c-TF-IDF calculations between the token sets and all topics. Due to its bag-of-word representation, this is quite fast. However, you might want to use the selected embedding_model instead to do this comparison. Do note that due to the many token sets, it is often computationally quite a bit slower:\n",
        "\n",
        "```python\n",
        "topic_distr, _ = topic_model.approximate_distribution(docs, use_embedding_model=True)\n",
        "```\n",
        "***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zegs-RlGLIO"
      },
      "source": [
        "## **Outlier Reduction**\n",
        "By default, HDBSCAN generates outliers which is a helpful mechanic in creating accurate topic representations. However, you might want to assign every single document to a topic. We can use `.reduce_outliers` to map some or all outliers to a topic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWhlaE3OF-9b"
      },
      "outputs": [],
      "source": [
        "# Reduce outliers\n",
        "new_topics = topic_model.reduce_outliers(abstracts, topics)\n",
        "\n",
        "# Reduce outliers with pre-calculate embeddings instead\n",
        "new_topics = topic_model.reduce_outliers(abstracts, topics, strategy=\"embeddings\", embeddings=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjmXkGQZGZKQ"
      },
      "source": [
        "**ðŸ’¡  NOTE - Update Topics with Outlier Reduction ðŸ’¡**\n",
        "***\n",
        "After having generated updated topic assignments, we can pass them to BERTopic in order to update the topic representations:\n",
        "\n",
        "```python\n",
        "topic_model.update_topics(docs, topics=new_topics)\n",
        "```\n",
        "\n",
        "It is important to realize that updating the topics this way may lead to errors if topic reduction or topic merging techniques are used afterwards. The reason for this is that when you assign a -1 document to topic 1 and another -1 document to topic 2, it is unclear how you map the -1 documents. Is it matched to topic 1 or 2.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7moKs4b5ANUF"
      },
      "source": [
        "## **Visualize Topics**\n",
        "\n",
        "With visualizations, we are closing into the realm of subjective \"best practices\". These are things that I generally do because I like the representations but your experience might differ.\n",
        "\n",
        "Having said that, there are two visualizations that are my go-to when visualizing the topics themselves:\n",
        "\n",
        "* `topic_model.visualize_topics()`\n",
        "* `topic_model.visualize_hierarchy()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9kRc7MD-XMD"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_topics(custom_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRgjSV_sFQtM"
      },
      "outputs": [],
      "source": [
        "topic_model.visualize_hierarchy(custom_labels=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBEtZ_P7A0ml"
      },
      "source": [
        "## **Visualize Documents**\n",
        "\n",
        "When visualizing documents, it helps to have embedded the documents beforehand to speed up computation. Fortunately, we have already done that as a \"best practice\".\n",
        "\n",
        "Visualizing documents in 2-dimensional space helps in understanding the underlying structure of the documents and topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsYYwd2pDLBo"
      },
      "outputs": [],
      "source": [
        "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
        "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u03NIcZdGGC-"
      },
      "source": [
        "The following plot is **interactive** which means that you can zoom in, double click on a label to only see that one and generally interact with the plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raUhiB9YBCCY"
      },
      "outputs": [],
      "source": [
        "# Visualize the documents in 2-dimensional space and show the titles on hover instead of the abstracts\n",
        "# NOTE: You can hide the hover with `hide_document_hover=True` which is especially helpful if you have a large dataset\n",
        "topic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, custom_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91_cyh9MGOaC"
      },
      "outputs": [],
      "source": [
        "# We can also hide the annotation to have a more clear overview of the topics\n",
        "topic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, custom_labels=True, hide_annotations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVhoKO4ABEuP"
      },
      "source": [
        "**ðŸ’¡  NOTE - 2-dimensional space ðŸ’¡**\n",
        "***\n",
        "Although visualizing the documents in 2-dimensional gives an idea of their underlying structure, there is a risk involved.\n",
        "\n",
        "Visualizing the documents in 2-dimensional space means that we have lost significant information since the original embeddings were more than 384 dimensions. Condensing all that information in 2 dimensions is simply not possible. In other words, it is merely an **approximation**, albeit quite an accurate one.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVk0FPLu7v2r"
      },
      "source": [
        "## **Serialization**\n",
        "\n",
        "When saving a BERTopic model, there are several ways in doing so. You can either save the entire model with `pickle`, `pytorch`, or `safetensors`.\n",
        "\n",
        "Personally, I would advise going with `safetensors` whenever possible. The reason for this is that the format allows for a very small topic model to be saved and shared.\n",
        "\n",
        "When saving a model with `safetensors`, it skips over saving the dimensionality reduction and clustering models. The `.transform` function will still work without these models but instead assign topics based on the similarity between document embeddings and the topic embeddings.\n",
        "\n",
        "As a result, the `.transform` step might give different results but it is generally worth it considering the smaller and significantly faster model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jx2uQDK8fHU"
      },
      "outputs": [],
      "source": [
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "topic_model.save(\"my_model_dir\", serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrRmrIDp8jLL"
      },
      "source": [
        "**ðŸ’¡  NOTE - Embedding Model ðŸ’¡**\n",
        "***\n",
        "Using `safetensors`, we are not saving the underlying embedding model but merely a pointer to the model. For example, in the above example we are saving the string `\"sentence-transformers/all-MiniLM-L6-v2\"` so that we can load in the embedding model alongside the topic model.\n",
        "\n",
        "This currently only works if you are using a sentence transformer model. If you are using a different model, you can load it in when loading the topic model like this:\n",
        "\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Define embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load model and add embedding model\n",
        "loaded_model = BERTopic.load(\"path/to/my/model_dir\", embedding_model=embedding_model)\n",
        "```\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvdYM4AA4Pjy"
      },
      "source": [
        "As mentioned above, loading can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUH8Jhbs4RuG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Define embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load model and add embedding model\n",
        "loaded_model = BERTopic.load(\"my_model_dir\", embedding_model=embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3aN-f9B4rmU"
      },
      "source": [
        "## **Inference**\n",
        "\n",
        "To speed up the inference, we can leverage a \"best practice\" that we used before, namely serialization. When you save a model as `safetensors` and then load it in, we are removing the dimensionality reduction and clustering steps from the pipeline.\n",
        "\n",
        "Instead, the assignment of topics is done through cosine similarity of document embeddings and topic embeddings. This speeds up inferences significantly.\n",
        "\n",
        "To show its effect, let's start by disabling the logger:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF0tIuMy5qSR"
      },
      "outputs": [],
      "source": [
        "from bertopic._utils import MyLogger\n",
        "logger = MyLogger(\"ERROR\")\n",
        "loaded_model.verbose = False\n",
        "topic_model.verbose = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbfYD4Sq53z-"
      },
      "source": [
        "Then, we run inference on both the loaded model and the non-loaded model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c49MiOj24WGF"
      },
      "outputs": [],
      "source": [
        "%timeit loaded_model.transform(abstracts[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMlk3lkR5Ofd"
      },
      "outputs": [],
      "source": [
        "%timeit topic_model.transform(abstracts[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGkChwWj6JLE"
      },
      "source": [
        "**1000 documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcmGXdP056t-"
      },
      "outputs": [],
      "source": [
        "%timeit loaded_model.transform(abstracts[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL6RdmAx58IG"
      },
      "outputs": [],
      "source": [
        "%timeit topic_model.transform(abstracts[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgH0Ut1j6SsT"
      },
      "source": [
        "**10_000 documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys-50O1a6OVX"
      },
      "outputs": [],
      "source": [
        "%timeit loaded_model.transform(abstracts[:10000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJm7KYU76QVT"
      },
      "outputs": [],
      "source": [
        "%timeit topic_model.transform(abstracts[:10000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBWDH0x35XkX"
      },
      "source": [
        "Based on the above, the `loaded_model` seems to be quite a bit faster for inference than the original `topic_model`."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}